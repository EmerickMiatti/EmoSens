{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3da2c54",
   "metadata": {},
   "source": [
    "## 1. Préparation des données\n",
    "On charge les textes et labels, puis on prépare les entrées pour le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09346bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import keras_nlp\n",
    "\n",
    "# Charger GoEmotions\n",
    "dataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\n",
    "train_hf = dataset[\"train\"]\n",
    "val_hf = dataset[\"validation\"]\n",
    "\n",
    "train_texts = list(train_hf[\"text\"])\n",
    "val_texts = list(val_hf[\"text\"])\n",
    "\n",
    "def convert_to_multilabel(example):\n",
    "    label_vector = np.zeros(28, dtype=np.float32)\n",
    "    for idx in example[\"labels\"]:\n",
    "        label_vector[idx] = 1.0\n",
    "    return label_vector\n",
    "\n",
    "train_labels = [convert_to_multilabel(ex) for ex in train_hf]\n",
    "val_labels = [convert_to_multilabel(ex) for ex in val_hf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756fbc4f",
   "metadata": {},
   "source": [
    "## 2. Modèle LSTM simple\n",
    "On entraîne un LSTM bidirectionnel sur les textes tokenisés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b29b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emerickmiatti/Development/Formation IA/Projet/Emosens/.venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 40ms/step - accuracy: 0.3254 - auc: 0.5708 - loss: 0.1517 - val_accuracy: 0.4119 - val_auc: 0.6655 - val_loss: 0.1312\n",
      "Epoch 2/3\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 42ms/step - accuracy: 0.4445 - auc: 0.7178 - loss: 0.1223 - val_accuracy: 0.4648 - val_auc: 0.7579 - val_loss: 0.1171\n",
      "Epoch 3/3\n",
      "\u001b[1m1357/1357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 42ms/step - accuracy: 0.4896 - auc: 0.8015 - loss: 0.1082 - val_accuracy: 0.4888 - val_auc: 0.7854 - val_loss: 0.1100\n",
      "LSTM - AUC validation : 0.7854 | Accuracy : 0.4888\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "max_len = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_texts), maxlen=max_len)\n",
    "X_val = pad_sequences(tokenizer.texts_to_sequences(val_texts), maxlen=max_len)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(max_words, 64, input_length=max_len),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=False)),\n",
    "    tf.keras.layers.Dense(28, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[tf.keras.metrics.AUC(name=\"auc\", multi_label=True), \"accuracy\"]\n",
    ")\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_auc_lstm = history_lstm.history['val_auc'][-1]\n",
    "val_acc_lstm = history_lstm.history['val_accuracy'][-1]\n",
    "print(f\"LSTM - AUC validation : {val_auc_lstm:.4f} | Accuracy : {val_acc_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6abe4f",
   "metadata": {},
   "source": [
    "## Conclusion comparative : BERT Small vs LSTM\n",
    "\n",
    "BERT Small surpasse donc légèrement le LSTM en AUC et offre de meilleures métriques de précision et de rappel, ce qui confirme la supériorité des modèles Transformers pour la classification multi-label d’émotions sur texte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7decd3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
